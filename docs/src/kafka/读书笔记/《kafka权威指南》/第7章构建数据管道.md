# 第7章 构建数据管道

在使用Kafka构建数据管道时，通常有2种使用场景：

- 把Kafka作为数据管道的两个端点之一。例如，把kafka里的数据移动到S3上，或者把MongoDB里的数据移动到kafka里。
- 把kafka作为数据管道两个端点的中间媒介。例如，为了把twitter的数据移动到elasticsearch，需要先把它们移动到kafka里，再将它们从kafka移动到elasticsearch上。

Kafka 为数据管道带来的主要价值在于，它可以作为数据管道各个数据段之间的大型缓冲区， 有效地解耦管道数据的生产者和消费者。 **Kafka 的解耦能力以及在安全和效率方面的可靠性** ，使它成为构建数据管道的最佳选择。

## 7.1 构建数据管道时需要考虑的问题

### 7.1.1 及时性

Kafka 作为一个基于流的数据平台，提供了可靠且可伸缩的数据存储，可以**支持几近实时的数据管道和基于小时的批处理**。生产者可以频繁地向 Kafka 写入数据，也可以按需写入；消费者可以在数据到达的第一时间读取它们，也可以每隔 一段时间读取一次积压的数据。Kafka 在这里扮演了一个大型缓冲区的角色，降低了生产者和消费者之间的时间敏感度。**实时的生产者和基于批处理的消费者可以同时存在，也可以任意组合**。实现回压策略也因此变得更加容易， Kafka 本身就使用了回压策略（必要时可以延后向生产者发送确认），消费速率完全取决于消费者自己。

### 7.1.2 可靠性

有些系统允许数据丢失，不过在大多数情况下，它们要求**至少一次传递**。也就是说，源系统的每一个事件都必须到达目的地，不过有时候需要进行重试，而重试可能造成重复传递。有些系统甚至要求**仅一次传递**一一源系统的每一个事件都必须到达目的地，不允许丢失，也不允许重复。

Kafka 本身就支持“至少一次传递”，如果再结合具有事务模型或唯一键特性的外部存储系统， Kafka 也能实现“仅一次传递”。因为大部分的端点都是数据存储系统，它们提供了“仅一次传递”的原语支持，所以基于 Kafka 的数据管道也能实现“仅一次传递”。

### 7.1.3 高吞吐量和动态吞吐量

由于我们将 Kafka 作为生产者和消费者之间的缓冲区，消费者的吞吐量和生产者的吞吐量就不会辑合在一起了。我们也不再需要实现复杂的回压机制，如果生产者的吞吐量超过了消费者的吞吐量，可以把数据积压在 Kafka 里，等待消费者追赶上来。**通过增加额外的消费者或生产者可以实现 Kafka 伸缩，因此我们可以在数据管道的任何一边进行动态的伸缩，以便满足持续变化的需求**。

因为 **Kafka 是一个高吞吐量的分布式系统，一个适当规模的集群每秒钟可以处理数百兆的数据**，所以根本无需担心数据管道无法满足伸缩性需求。

### 7.1.4 数据格式

数据管道需要协调各种数据格式和数据类型，这是数据管道 一个非常重要的因素。数据类型取决于不同的数据库和数据存储系统。你可能会通过 Avro将XML 或关系型数据加载到Kafka 里，然后将它们转成 JSON 写入ElasticSearch ，或者转成 Parquet 写入HDFS ，或者转成 csv 写入 S3。

### 7.1.5 转换

### 7.1.6 安全性

Kafka 支持加密传输数据，从数据源到 Kafka ，再从 Kafka 到数据池。它还支持认证（通过SASL 来实现）和授权，如果 一个主题包含了敏感信息，在不经授权的情况下，数据是不会流到不安全的系统里的。 Kafka 还提供了审计日志用于跟踪访问记录。通过编写额外的代码，还可能跟踪到每个事件的来源和事件的修改者，从而在每个记录之间建立起整体的联系。

### 7.1.7 故障处理能力

### 7.1.8 耦合性和灵活性

