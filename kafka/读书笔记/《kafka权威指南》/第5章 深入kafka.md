# 第5章 深入kafka

- kafka如何进行复制
- kafka如何处理来自生产者和消费者的请求
- kafka的存储细节，比如文件格式和索引

## 5.1 集群成员关系

kafka使用zookeeper来维护集群成员之间的信息。每个broker都有一个唯一的标识符，这个标识符可以在配置文件里指定，也可以自动生成。在broker启动的时候，它通过创建**临时节点**把自己的ID注册到zookeeper。kafka组件订阅zookeeper的/brokers/ids路径（broker在zookeeper上的注册路径），当有broker加入集群或退出集群时，这些组件就可以获得通知。

如果要启动另一个具有相同ID的broker，会得到一个错误-----新broker会试着进行注册但不会成功，因为zookeeper里已经有一个具有相同ID的broker。

在broker停机、出现网络分区或长时间垃圾回收停顿时，broker会从zookeeper上断开连接，此时broker在启动时创建的临时节点会自动从zookeeper上移除。监听broker列表的kafka组件会被告知该broker已移除。

## 5.2 控制器

控制器其实就是 broker ，它除了具有一般 broker 的功能之外，还**负责分区首领的选举**。集群里第一个启动的 broker通过在Zookeeper里创建一个**临时节点** /controller 让自己成为控制器。其它broker在启动时也会**尝试创建**这个节点，不过它们会收到一个“节点已存在”的异常，然后“意识”到控制器节点已存在，也就是集群里已经有一个控制器了。其他 broker 在控制器节点上创建Zookeeper watch 对象，这样它们就可以收到这个节点的变更通知。这种方式可以确保集群里一次只有一个控制器存在。

如果控制器**被关闭**或者与zookeeper**断开连接**，zookeeper上的临时节点就会消失。集群里的其他broker通过watch对象得到控制器节点消失的通知，它们会尝试让自己成为新的控制器。第一个在zookeeper里成功创建控制器节点的broker就会成为新的控制器，其他节点会收到"节点已存在"的异常，然后在新的控制器节点上再次创建watch对象。每个新选出的控制器通过zookeeper的条件递增操作获得一个全新的、数值更大的controller epoch，其他broker在知道当前controller epoch后，如果收到由控制器发出的包含较旧epoch的消息，就会忽略它们。

当控制器发现一个broker 已经离开集群（通过观察相关的 Zookeeper 路径），它就知道，那些失去首领的分区需要一个新首领（这些分区的首领刚好是在这个 broker 上）。控制器遍历这些分区，并确定谁应该成为新首领（简单来说就是分区副本列表里的下一个副本），然后向所有包含新首领或现有跟随者的 broker 发送请求。该请求消息包含了**谁是新首领**以及**谁是分区跟随者**的信息。随后，新首领开始处理来自生产者和消费者的请求，而跟随者开始从新首领那里复制消息。

当控制器发现一个 broker 加入集群时，它会使用 broker ID来检查新加入的 broker 是否包含现有分区的副本。如果有，控制器就把变更通知发送给新加入的 broker 和其他 broker, 新broker 上的副本开始从首领那里复制消息。

简而言之， Kafka 使用 Zookeeper 的临时节点来选举控制器， 并在节点加入集群或退出集群时通知控制器。控制器负责在节点加入或离开集群时进行分区首领选举。控制器使用epoch 来避免“脑裂” 。“脑裂”是指两个节点同时认为自己是当前的控制器。

## 5.3 复制

复制功能是 Kafka 架构的核心。在 Kafka 的文档里， Kafka 把自己描述成“ 一个分布式的可分区的、可复制的提交日志服务”。

Kafka使用主题来组织数据，每个主题被分为若干个分区，每个分区有多个副本。那些副本被保存在broker上，每个broker可以保存**成百上千**个属于不同主题和分区的副本。副本有以下两种类型。

**首领副本**

每个分区都有一个首领副本。为了保证一致性，所有生产者请求和消费者请求都会经过这个副本。

**跟随者副本**

首领以外的副本都是跟随者副本。跟随者副本不处理来自客户端的请求，它们唯一的任务就是**从首领那里复制消息，保持与首领一致的状态**。如果首领发生崩渍，其中的一个跟随者会被提升为新首领。

首领的**另一个任务是搞清楚哪个跟随者的状态与自己是一致的**。跟随者为了保持与首领的状态一致，在有新消息到达时尝试从首领那里复制消息，不过有各种原因会导致同步失败，例如，**网络拥塞导致复制变慢**，**broker发生崩溃导致复制滞后**，直到重启 broker 后复制才会继续。

为了与首领保持同步，跟随者向首领发送获取数据的请求，这种请求与消费者为了读取悄息而发送的请求是一样的。首领将响应消息发给跟随者。请求消息里包含了跟随者想要获取消息的偏移量，而且这些偏移量总是有序的。

一个跟随者副本先请求淌息1，接着请求消息2，然后请求消息3，在收到这3个请求的响应之前，它是不会发送第4个请求消息的。如果跟随者发送了请求消息 ，那么首领就知道它已经收到了前面3个请求的响应。通过查看每个跟随者请求的最新偏移 ，首领就会知道每个跟随者复制的进度。如果跟随者在 **10 内**没有请求任何消息，或者虽然在请求消息，但在 10s 内没有请求最新的数据，那么它就会被认为是**不同步**的。如果一个副本无法与首领保持一致，在首领发生失效时，它就**不可能成为新首领**一一毕竟它没有包含全部的消息。

相反，**持续请求得到的最新消息副本被称为同步的副本**。在首领失效时，只有同步副本才有可能被选为新首领。

除了当前首领之外，每个分区都有一个首选首领---创建主题时选定的首领就是分区的首选首领。之所以把它叫作首选首领，是因为在创建分区时，需要在broker 之间均衡首领。因此，我们希望首选首领在成为真正的首领时， broker 间的负载最终会得到均衡。默认情况下 auto.leader.rebalance.enable 被设为 true ，它会检查首选首领是不是当前首领，如果不是，并且该副本是同步的，那么就会触发首领选举，让首选首领成为当前首领。

分区的副本列表里的第一个副本就是首选首领。

## 5.4 处理请求

broker 的大部分工作是处理**客户端、分区副本和控制器**发送给分区首领的请求。 Kafka提供了一个二进制协议（基于 TCP ），指定了请求消息的格式以及 broker 如何对请求作出响应一一包括成功处理请求或在处理请求过程中遇到错误。客户端发起连接并发送请求，broker 处理请求井作出响应。 **broker 按照请求到达的顺序来处理它们**一一这种顺序保证让Kafka 具有了消息队列的特性，同时保证保存的消息也是有序的。

broker 会在它所监听的每一个端口上运行 **Acceptor线程**，这个线程会创建一个连接，并把它交给 **Processor 线程**去处理。 Processor 线程（也被叫作“**网络线程**”）的数量是可配置的。网络线程负责从客户端获取请求消息，把它们放进**请求队列**，**IO线程**会负责处理请求，然后从**响应队列**取响应消息，把它们发送给客户端。下图是处理请求的内部流程。

![](/Users/gaobo/Documents/Typora/typora-pic/iShot2021-11-28 20.25.07-8102461.png)

两种常见的请求类型：

**生产请求**

生产者发送的请求，它包含客户端要写入 broker 的消息。

**获取请求**

在消费者和跟随者副本需要从 broker 读取消息时发送的请求。

生产请求和获取请求都必须发送给分区的首领副本。如果 broker 收到一个针对特定分区的请求，而该分区的首领在另一个broker 上，那么发送请求的客户端会收到一个“非分区首领”的错误响应。

那么客户端怎么知道该往哪里发送请求呢？客户端使用了另一种请求类型，也就是**元数据请求**。这种请求包含了客户端感兴趣的**主题列表**。服务器端的响应消息里指明了这些主题所包含的分区、每个分区都有哪些副本， 以及哪个副本是首领。元数据请求可以发送给**任意一个 broker** ，因为所有 broker 都缓存了这些信息。

一般情况下，客户端会把这些信息缓存起来，并直接往目标 broker 上发送生产请求和获取请求。它们需要**时不时地通过发送元数据请求来刷新这些信息**（刷新的时间间隔通过metadata.max.age.ms 参数来配置），从而知道元数据是否发生了变更一一－比如，在新broker 加入集群时，部分副本会被移动到新 broker 上，如下图所示。另外，如果客户端收到“非首领”错误，它会在尝试重发请求之前先刷新元数据，因为这个错误说明了客户端正在使用过期的元数据信息，之前的请求被发到了错误的 broker上。

![image-20211128205923217](https://typora-gao-pic.oss-cn-beijing.aliyuncs.com/image-20211128205923217.png)

### 5.4.1 生产请求

该参数指定了需要多少个 broker 确认才可以认为一个消息写入是成功的。不同的配置对“写入成功”的界定是不一样的，如果 acks=1 ，那么只要首领收到消息就认为写入成功；如果aks=all ，那么需要所有同步副本收到消息才算写入成功；如果 acks=0，那么生产者在把消息发出去之后，完全不需要等待 broker 的响应。

包含首领副本的broker在收到生产请求时，会对请求做一些验证：

- 发送数据的用户是否有主题写入权限
- 请求里包含的acks值是否有效（只允许出现0、1、all）？
- 如果acks=all，是否有足够多的同步副本

之后，消息被**写入本地磁盘**。在 Linux 系统上，消息会被写到**文件系统缓存**里，并不保证它们何时会被刷新到磁盘上。 Kafka 不会一直等待数据被写到磁盘上一一它依赖复制功能来保证消息的持久性。

在消息被写入分区的首领之后， broker 开始检查acks 配置参数一一如果 acks 被设为0或1,那么 broker 立即返回响应；如果 acks 被设为 all ，那么请求会被保存在一个叫作**炼狱**的缓冲区里，直到首领发现所有跟随者副本都复制了消息，晌应才会被返回给客户端。

### 5.4.2 获取请求

broker 处理获取请求的方式与处理生产请求的方式很相似。客户端发送请求，向 broker请求主题分区里具有特定偏移量的消息，好像在说 “请把主题 Test 分区0偏移量从 53 开始的消息以及主题 Test 分区3偏移量从 64 开始的消息发给我。”客户端还可以指定 broker最多可以从一个分区里返回多少数据。这个限制是非常重要的，因为客户端需要为 broker返回的数据分配足够的内存。如果没有这个限制，broker 返回的大量数据有可能耗尽客户端的内存。

broker 将按照客户端指定的数量上限从分区里读取消息，再把消息返回给客户端。Kafka 使用**零复制**技术向客户端发送消息一一也就是说， Kafka 直接把消息从文件（或者更确切地说是 Li ux 文件系统缓存）里发送到网络通道，而不需要经过任何中间缓冲区。这是 Kafka 与其他大部分数据库系统不一样的地方，其他数据库在将数据发送给客户端之前会先把它们保存在本地缓存里。这项技术**避免了字节复制，也不需要管理内存缓冲区，从而获得更好的性能。**

客户端除了可以设置broker返回的数据的上限，也可以设置下限。在主题消息流量不是很大的情况下，这样可以**减少 CPU 和网络开销**。客户端发送一个请求，broker等到有足够的数据时才把它们返回给客户端，然后客户端再发出请求，而不是让客户端每隔几毫秒就发送 次请求，每次只能得到很少的数据甚至没有数据。对比这两种情况，它们最终读取的数据总量是一样的，但前者的来回传送次数更少，因此开销也更小。

![image-20211201003832720](https://typora-gao-pic.oss-cn-beijing.aliyuncs.com/image-20211201003832720.png)

我们不会让客户端一直等待 broker 累积数据。在等待了一段时间之后，就可以把可用的数据拿回处理，而不是一直等待下去。所以，客户端可以定义一个**超时时间**，告诉broker ：“如果你无法在超时时间内累积满足要求的数据量，那么就把当前这些数据返回给我。“

并不是所有保存在分区首领上的数据都可以被客户端读取。**大部分客户端只能读取已经被写入所有同步副本的消息**（跟随者副本也不行，尽管它们也是消费者---否则复制功能就无法工作）。分区首领知道每个消息会被复制到哪个副本上，在消息还没有被写入所有同步副本之前，是不会发送给消费者的一一尝试获取这些消息的请求会得到**空的响应**而不是错误。

如果首领发生崩溃，另一个副本成为新首领，那么这些消息就丢失了。如果我们允许消费者读取这些消息，可能就会破坏一致性。

### 5.4.3 其他请求

客户端在网络上使用的通用二进制协议。broker之间也使用同样的通信协议。

## 5.5 物理存储

Kafka的基本存储单元是分区。

### 5.5.1 分区分配

在创建主题时，Kafka首先会决定如何在broker间分配分区。在进行分区分配时，我们要达到如下的目标：

- 在broker间平均地分布分区副本
- 确保每个分区的每个副本分布在不同的broker上
- 如果为broker指定了机架信息，那么尽可能把每个分区的副本分配到不同机架的 broker上。这样做是为了保证一个机架的不可用不会导致整体的分区不可用。

### 5.5.2 文件管理

保留数据是 Kafka 的一个基本特性， Kafka 不会一直保留数据，也不会等到所有消费者都读取了消息之后才删除消息。相反， Kafka 管理员为每个主题配置了数据保留期限，规定数据被删除之前可以保留多长时间，或者清理数据之前可以保留的数据量大小。

因为在一个大文件里查找和删除消息是很费时的，也很容易出错，所以我们把分区分成若干个片段。默认情况下，每个片段包含 lGB 或一周的数据，以较小的那个为准。在 broker往分区写入数据时，如果达到片段上限，就关闭当前文件，井打开一个新文件。当前正在写入数据的片段叫作**活跃片段**。活动片段永远不会被删除，所以如果你要保留数据，但片段里包含了5天的数据，那么这些数据会被保留5天，因为在片段被关闭之前这些数据无法被删除。如果你要保留数据一周，而且每天使用一个新片段，每天在使用一个新片段的同时会删除 个最老的片段一一－所以大部分时间该分区会有7个片段存在。

### 5.5.3 文件格式

Kafka 的消息和偏移量保存在文件里。保存在磁盘上的数据格式与从生产者发送过来或者发送给消费者的消息格式是一样的。因为使用了相同的消息格式进行磁盘存储和网络传输，Kafka 可以使用零复制技术给消费者发送消息，同时避免了对生产者已经压缩过的消息进行解压和再压缩。

